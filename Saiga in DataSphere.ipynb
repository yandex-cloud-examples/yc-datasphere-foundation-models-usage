{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9975f93b-0875-4fa1-90cf-8c766445b213",
   "metadata": {},
   "source": [
    "# LLM Saiga/Mistral 7B, Russian Mistral-based chatbot в DataSphereв DataSphere\n",
    "Автор модели [IlyaGusev](https://huggingface.co/IlyaGusev).\n",
    "\n",
    "На базе [Mistral OpenOrca](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed4b6a-2dce-4d43-9654-60814129b749",
   "metadata": {},
   "source": [
    "### Требования\n",
    "\n",
    "1. 30 ГБ свободного места в хранилище проекта.\n",
    "2. Минимальная конфигурация вычислительных ресурсов — g1.1 (1 карта GPU NVIDIA® Tesla® V100).\n",
    "3. Токен доступа от huggingface.co, сохраненный в проекте в секрете `huggingface_key`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415b1ac-80de-4b81-b869-cf09764f1e49",
   "metadata": {},
   "source": [
    "### Устанавливаем библиотеки\n",
    "\n",
    "После выполнения ячейки перезагрузите ядро (**Kernel -> Reset Kernel**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73476e-dc1c-4dbd-8e16-183ebca37969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install accelerate bitsandbytes peft transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008d51e-1c77-4ac0-8835-3049214051b6",
   "metadata": {},
   "source": [
    "### Задаем путь для сохранения кешированных данных модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67dd15-5ce9-492c-9ade-ab4c888cc05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/jupyter/datasphere/project/modelcache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce371d2-c84b-412a-a2a6-8c1c4b35c336",
   "metadata": {},
   "source": [
    "### Определяем токен для доступа к huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3513175-9161-4bd5-9dae-2c993b741e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_token = os.environ['huggingface_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd78c5e-9dd1-4f97-8bba-5b4bd2fc800d",
   "metadata": {},
   "source": [
    "### Определяем и скачиваем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8535c-c4d1-4204-8d2f-b51deaa19f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.<s>system\\n\"\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a5d2c",
   "metadata": {
    "cellId": "3nyhedgy2qwypo9qitzxs",
    "execution_id": "f1ed7e73-dc07-4a79-b2e9-29021d2bcc1c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(MODEL_NAME, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.float32, device_map=\"auto\", token=access_token)\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float32, token=access_token)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba93fce-55a8-4e3c-a32f-0aa7214c9218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, token=access_token)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME, token=access_token)\n",
    "#print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d788b2-f909-44f8-959e-1dd16c59b5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20724e7e-b460-450e-a8c7-533d7b27b5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35062687-0709-49c0-8d4a-45bae3639534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T15:29:29.036183Z",
     "iopub.status.busy": "2024-02-07T15:29:29.034977Z",
     "iopub.status.idle": "2024-02-07T15:29:29.049676Z",
     "shell.execute_reply": "2024-02-07T15:29:29.048535Z",
     "shell.execute_reply.started": "2024-02-07T15:29:29.036137Z"
    },
    "tags": []
   },
   "source": [
    "### Делаем запрос к модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75dff1",
   "metadata": {
    "cellId": "qolgcczzcnl0hi2c0m97",
    "execution_id": "09337bcd-b2ae-4f69-9d47-0aae81f4c775",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = [\"Сочини рассказ, обязательно упоминая следующие объекты. Дано: дальняя дорога, поля, автомобиль и новые технологии\", \"Почему трава зеленая?\"]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(prompt)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787e4b5-8011-4c78-867f-9c137ab25f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "notebookId": "d8512373-ee04-4db5-827c-f65b7465263f",
  "notebookPath": "LLM/Gusev.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
